# Documentation Downloader

A universal documentation downloader that crawls documentation websites and converts them to organized markdown files.

## Features

- ğŸ•·ï¸ **Web Crawling**: Automatically discovers and downloads entire documentation sites
- ğŸ“ **Markdown Conversion**: Converts HTML pages to clean markdown format
- ğŸ“ **Smart Organization**: Organizes files in logical folder structures
- ğŸ”„ **Markdown Detection**: Prioritizes existing markdown files over HTML conversion
- âš™ï¸ **Configurable**: Site-specific configuration for optimal content extraction
- ğŸš€ **Bulk Downloads**: Download multiple documentation sites at once
- ğŸ’¾ **Incremental Updates**: Skip existing files unless forced to re-download

## Installation

```bash
npm install
```

## Usage

### Recommended usage
First, install claude code.

Then,
```bash
claude --dangerously-skip-permissions
Claude mom i want docs.example.com, please!
```

### Single Site Download

```bash
npm run download -- --url https://docs.example.com --output ./downloads --depth 3
```

### Bulk Download from env.md

Download all documentation sites listed in your env.md file:

```bash
npm run download -- bulk --file ../env.md --output ./downloads --depth 3
```

### Command Line Options

#### Single Download
- `-u, --url <url>`: Documentation website URL to download (required)
- `-o, --output <dir>`: Output directory (default: ./downloads)
- `-d, --depth <number>`: Maximum crawl depth (default: 3)
- `--force`: Force re-download even if files exist
- `--config <file>`: Configuration file for site-specific settings

#### Bulk Download
- `-f, --file <file>`: Environment file with URLs (default: ../env.md)
- `-o, --output <dir>`: Output directory (default: ./downloads)
- `-d, --depth <number>`: Maximum crawl depth (default: 3)
- `--force`: Force re-download even if files exist

## Configuration

Create a `config.json` file to customize behavior for specific documentation sites:

```json
{
  "docs.example.com": {
    "contentSelector": ".markdown-body, .content, main",
    "skipPatterns": ["/api/", "/changelog"],
    "maxDepth": 4
  }
}
```

### Configuration Options

- `contentSelector`: CSS selectors to extract main content
- `skipPatterns`: URL patterns to skip during crawling
- `maxDepth`: Maximum crawl depth for this specific site

## Output Structure

Downloaded documentation is organized by site:

```
downloads/
â”œâ”€â”€ docs_sim_dune_com/
â”‚   â”œâ”€â”€ index.md
â”‚   â”œâ”€â”€ quickstart.md
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ reference.md
â”œâ”€â”€ www_alchemy_com/
â”‚   â”œâ”€â”€ index.md
â”‚   â””â”€â”€ docs/
â”‚       â””â”€â”€ reference/
â”‚           â””â”€â”€ data-overview.md
â””â”€â”€ ...
```

Each markdown file includes metadata:

```markdown
---
source_url: https://docs.example.com/page
downloaded_at: 2024-01-15T10:30:00.000Z
---

# Page Content
...
```

## Supported Sites

The downloader works with most documentation sites. Pre-configured for common API documentation formats including:

- Static site generators (GitBook, Docusaurus, VitePress)
- API documentation platforms 
- Developer documentation sites
- Knowledge bases and wikis

## How It Works

1. **URL Discovery**: Starts from a base URL and crawls internal links
2. **Content Extraction**: Uses CSS selectors to extract main content
3. **Markdown Detection**: Checks for existing markdown versions first
4. **HTML Conversion**: Converts HTML to markdown using Turndown
5. **File Organization**: Saves files in organized directory structure
6. **Metadata**: Adds source URL and download timestamp to each file

## Limitations

- Respects robots.txt and rate limiting
- Only downloads from the same domain as the starting URL
- Maximum crawl depth prevents infinite loops
- Some dynamic content may not be captured

## Contributing

Feel free to submit issues and enhancement requests!